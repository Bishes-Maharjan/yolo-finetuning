{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNGNSW2N9ZmlRRhjsvRomyP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47PoQ8iv1rhd","executionInfo":{"status":"ok","timestamp":1756908085053,"user_tz":-345,"elapsed":24732,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}},"outputId":"97f39af8-e489-4a01-8510-09fd03b5cef2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.192-py3-none-any.whl.metadata (37 kB)\n","Collecting roboflow\n","  Downloading roboflow-1.2.7-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.8.3)\n","Collecting idna==3.7 (from roboflow)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n","Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting pi-heif<2 (from roboflow)\n","  Downloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n","Collecting pillow-avif-plugin<2 (from roboflow)\n","  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n","Collecting filetype (from roboflow)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.192-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading roboflow-1.2.7-py3-none-any.whl (88 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\n","Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, ultralytics-thop, roboflow, ultralytics\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.12.0.88\n","    Uninstalling opencv-python-headless-4.12.0.88:\n","      Successfully uninstalled opencv-python-headless-4.12.0.88\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.0 pillow-avif-plugin-1.5.2 roboflow-1.2.7 ultralytics-8.3.192 ultralytics-thop-2.0.16\n","Requirement already satisfied: roboflow in /usr/local/lib/python3.12/dist-packages (1.2.7)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.8.3)\n","Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n","Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.0)\n","Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.5.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.59.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.3)\n","loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in Face-Detection-1 to yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214621/214621 [00:02<00:00, 74445.42it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to Face-Detection-1 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3128/3128 [00:00<00:00, 4218.02it/s]\n"]}],"source":["!pip install ultralytics roboflow\n","!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"jDWLmA2sKK6F2G3s5NM2\")\n","project = rf.workspace(\"bishes\").project(\"face-detection-mik1i-8e409\")\n","version = project.version(1)\n","dataset = version.download(\"yolov5\")"]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","\n","model = YOLO('yolov8n.pt')\n","\n","results = model.train(\n","    data='Face-Detection-1/data.yaml',\n","    epochs=25,\n","    imgsz=640,\n","    batch=16,\n","    name='face-detection'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_F42U-D12BY","executionInfo":{"status":"ok","timestamp":1756908952152,"user_tz":-345,"elapsed":718311,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}},"outputId":"0e8e5a79-b0cf-49b3-9e44-57a977c74896"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.192 ðŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=Face-Detection-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=face-detection, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/face-detection, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n","Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n","\n","Transferred 319/355 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1869.5Â±1496.5 MB/s, size: 165.5 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Face-Detection-1/train/labels.cache... 1356 images, 235 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1356/1356 2055466.7it/s 0.0s\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 688.9Â±371.4 MB/s, size: 88.6 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Face-Detection-1/valid/labels.cache... 38 images, 1 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 17750.7it/s 0.0s\n","Plotting labels to runs/detect/face-detection/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/face-detection\u001b[0m\n","Starting training for 25 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       1/25         3G      1.429      2.121      1.362         66        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 2.8it/s 30.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.6it/s 0.6s\n","                   all         38         49      0.669      0.743      0.816       0.44\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       2/25         3G      1.416      1.546      1.314         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.1it/s 0.5s\n","                   all         38         49      0.924      0.748      0.897       0.52\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       3/25      3.02G      1.396      1.377        1.3         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.6it/s 0.4s\n","                   all         38         49       0.72      0.653      0.747      0.344\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       4/25      3.02G      1.369      1.257      1.309         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.3s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.5it/s 0.4s\n","                   all         38         49      0.887      0.816      0.895      0.466\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       5/25      3.02G      1.368      1.206      1.322         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.3it/s 0.6s\n","                   all         38         49      0.918      0.919      0.965      0.577\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       6/25      3.02G      1.259      1.073      1.254         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.5s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.4it/s 0.5s\n","                   all         38         49      0.898      0.959      0.938      0.558\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       7/25      3.02G      1.276      1.018      1.259         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.4it/s 0.5s\n","                   all         38         49      0.907      0.898      0.953      0.596\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       8/25      3.02G      1.234     0.9785      1.237         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.0it/s 0.5s\n","                   all         38         49      0.871      0.816      0.908      0.506\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       9/25      3.02G      1.219     0.9399      1.216         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.0it/s 0.5s\n","                   all         38         49       0.94      0.918      0.966      0.662\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      10/25      3.04G      1.195     0.9274      1.195         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.8it/s 0.7s\n","                   all         38         49      0.957      0.939      0.957      0.628\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      11/25      3.06G      1.157     0.8608      1.183         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.5it/s 0.4s\n","                   all         38         49      0.951      0.939      0.977      0.685\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      12/25      3.07G      1.158     0.8539      1.178         49        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.4s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.2it/s 0.5s\n","                   all         38         49      0.959      0.958      0.973      0.632\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      13/25      3.09G      1.124     0.8062       1.15         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.7it/s 0.4s\n","                   all         38         49      0.975      0.918      0.967      0.706\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      14/25      3.11G      1.111     0.8084       1.15         64        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.5s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.9it/s 0.5s\n","                   all         38         49      0.957      0.916      0.965      0.581\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      15/25      3.11G      1.084     0.7748       1.14         67        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.1it/s 27.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.9it/s 0.7s\n","                   all         38         49      0.942      0.918      0.969      0.696\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      16/25      3.11G       1.05     0.7475       1.12         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.0it/s 28.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.2it/s 0.5s\n","                   all         38         49          1      0.934      0.973       0.69\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      17/25      3.11G      1.002      0.681      1.087         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.5s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.7it/s 0.4s\n","                   all         38         49      0.925      0.959       0.98      0.666\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      18/25      3.11G     0.9953      0.651       1.09          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.2s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.3it/s 0.5s\n","                   all         38         49      0.935      0.959      0.976       0.71\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      19/25      3.11G     0.9978     0.6518      1.087         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.6s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.4it/s 0.6s\n","                   all         38         49      0.979      0.959      0.977      0.724\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      20/25      3.11G     0.9491     0.6112      1.057         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.8s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.7it/s 0.5s\n","                   all         38         49      0.959      0.951      0.979      0.712\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      21/25      3.11G     0.9265     0.6055      1.049         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.5s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.1it/s 0.5s\n","                   all         38         49      0.975      0.939       0.97      0.681\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      22/25      3.11G     0.9108     0.5872      1.039         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.4s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.3it/s 0.5s\n","                   all         38         49       0.95      0.959      0.974      0.722\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      23/25      3.11G     0.8797     0.5664      1.036         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.4s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.1it/s 0.4s\n","                   all         38         49      0.958      0.959      0.973      0.727\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      24/25      3.11G     0.8681     0.5449      1.028         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.2it/s 26.7s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.5it/s 0.4s\n","                   all         38         49      0.991      0.959      0.979      0.736\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      25/25      3.11G     0.8348     0.5289      1.001          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 85/85 3.3it/s 26.1s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.6it/s 0.4s\n","                   all         38         49      0.978      0.959      0.982      0.755\n","\n","25 epochs completed in 0.197 hours.\n","Optimizer stripped from runs/detect/face-detection/weights/last.pt, 6.2MB\n","Optimizer stripped from runs/detect/face-detection/weights/best.pt, 6.2MB\n","\n","Validating runs/detect/face-detection/weights/best.pt...\n","Ultralytics 8.3.192 ðŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.1it/s 0.4s\n","                   all         38         49      0.978      0.959      0.982      0.755\n","Speed: 0.4ms preprocess, 2.0ms inference, 0.0ms loss, 2.3ms postprocess per image\n","Results saved to \u001b[1mruns/detect/face-detection\u001b[0m\n"]}]},{"cell_type":"markdown","source":["*The pretrained model has generated the weights, now we use weights for our data set*"],"metadata":{"id":"sy-cGoqC8ZtT"}},{"cell_type":"code","source":["model = YOLO('./runs/detect/face-detection/weights/best.pt')"],"metadata":{"id":"SYdb6iC28ixx","executionInfo":{"status":"ok","timestamp":1756909214733,"user_tz":-345,"elapsed":40,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","val_results = model.val(data='Face-Detection-1/data.yaml')  # Uses 'val: ./valid/images'\n","print(f\"Validation mAP@0.5: {val_results.box.map50:.4f}\")\n","\n","# Test on test set (final evaluation)\n","test_results = model.val(data='Face-Detection-1/data.yaml', split='test')  # Uses 'test: ../test/images'\n","print(f\"Test mAP@0.5: {test_results.box.map50:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21G3CF698hyJ","executionInfo":{"status":"ok","timestamp":1756909710940,"user_tz":-345,"elapsed":10986,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}},"outputId":"bbbf621d-536a-4578-b699-136bc1678b43"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.192 ðŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1691.8Â±548.5 MB/s, size: 95.3 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Face-Detection-1/valid/labels.cache... 38 images, 1 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 81903.2it/s 0.0s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n","                   all         38         49      0.978      0.959      0.982      0.758\n","Speed: 7.2ms preprocess, 9.1ms inference, 0.0ms loss, 1.6ms postprocess per image\n","Results saved to \u001b[1mruns/detect/val3\u001b[0m\n","Validation mAP@0.5: 0.9818\n","Ultralytics 8.3.192 ðŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1590.3Â±487.3 MB/s, size: 117.7 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Face-Detection-1/test/labels... 164 images, 28 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 164/164 2602.4it/s 0.1s\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Face-Detection-1/test/labels.cache\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.1it/s 3.5s\n","                   all        164        263      0.921      0.905      0.963      0.682\n","Speed: 2.1ms preprocess, 7.5ms inference, 0.0ms loss, 2.1ms postprocess per image\n","Results saved to \u001b[1mruns/detect/val4\u001b[0m\n","Test mAP@0.5: 0.9626\n"]}]},{"cell_type":"code","source":["prediction = model.predict(source=\"Face-Detection-1/test/images\", save=True, conf=0.25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GD3_i9Lj9OJZ","executionInfo":{"status":"ok","timestamp":1756910473502,"user_tz":-345,"elapsed":6342,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}},"outputId":"f0853eab-5a05-4b92-8d88-ac209da11263"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/164 /content/Face-Detection-1/test/images/-I1-MS09uaqsLdGTFkgnS0Rcg1mmPyAj95ySg_eckoM_jpeg_jpg.rf.9025a19148e143cbfce045d4600d70a8.jpg: 448x640 5 faces, 44.3ms\n","image 2/164 /content/Face-Detection-1/test/images/0ad90195-cd77-489e-bf85-08c83b80d3e0_jpg.rf.1e896e55d75db806976babb64c70828c.jpg: 384x640 4 faces, 65.2ms\n","image 3/164 /content/Face-Detection-1/test/images/1125506397_15801322206131n_jpg.rf.5fe52a73c4181598dc381fb7c5df6472.jpg: 448x640 9 faces, 29.6ms\n","image 4/164 /content/Face-Detection-1/test/images/1196686205_jpg_14_jpg.rf.21a2751ea2900835a56f21bec48dde76.jpg: 448x640 5 faces, 38.5ms\n","image 5/164 /content/Face-Detection-1/test/images/1288788-une-employee-aide-des-voyageurs-en-provenance-de-chine-le-26-janvier-2020-a-l-aeroport-de-roissy_jpg.rf.f79e81c2f6f1822658fa2911ac9c688a.jpg: 448x640 7 faces, 11.1ms\n","image 6/164 /content/Face-Detection-1/test/images/1422808187-3291816118_jpg.rf.eb35d3e58f873adb1c700ec5379894ed.jpg: 544x640 1 face, 98.2ms\n","image 7/164 /content/Face-Detection-1/test/images/1482202839575_jpg.rf.e835f73c6b25813a5054d6884b13b6b4.jpg: 416x640 3 faces, 61.2ms\n","image 8/164 /content/Face-Detection-1/test/images/1483800496-3386248642_jpg.rf.e9892d26a3ba8711a9354705960d6ae7.jpg: 640x640 6 faces, 11.1ms\n","image 9/164 /content/Face-Detection-1/test/images/15-08608-001_jpg.rf.147bba25eced62232177d28024872172.jpg: 352x640 5 faces, 104.3ms\n","image 10/164 /content/Face-Detection-1/test/images/1553605632_9d5877d8_60_jpg.rf.46f90567a5fb6f429e3c365e652b70a7.jpg: 480x640 7 faces, 80.7ms\n","image 11/164 /content/Face-Detection-1/test/images/20221109_164231_jpg.rf.90660123b6e528c6003b0e8811646a73.jpg: 640x640 (no detections), 8.2ms\n","image 12/164 /content/Face-Detection-1/test/images/20221109_164330_jpg.rf.44525d415feb71a9ea9e94b74ac46b4f.jpg: 640x640 (no detections), 7.4ms\n","image 13/164 /content/Face-Detection-1/test/images/20221109_164420_jpg.rf.b394f82130bd57431c9abf07062912ea.jpg: 640x640 (no detections), 7.3ms\n","image 14/164 /content/Face-Detection-1/test/images/20221109_164524-1-_jpg.rf.b2f304f028284a932019090945069c7f.jpg: 640x640 (no detections), 7.4ms\n","image 15/164 /content/Face-Detection-1/test/images/20221109_164558_jpg.rf.0358f8a99836aef87adf325315463e49.jpg: 640x640 (no detections), 8.3ms\n","image 16/164 /content/Face-Detection-1/test/images/20221109_165214_jpg.rf.632a2c124edd5f64ddc76e6ace20c228.jpg: 640x640 (no detections), 14.5ms\n","image 17/164 /content/Face-Detection-1/test/images/2559_jpeg.rf.19cd2b0c6e1c33e810b62dfddb033257.jpg: 480x640 6 faces, 9.2ms\n","image 18/164 /content/Face-Detection-1/test/images/IMG_4920_mov-12_jpg.rf.bacdb47e10084a4c920b5a5548946b50.jpg: 640x384 1 face, 58.6ms\n","image 19/164 /content/Face-Detection-1/test/images/IMG_4920_mov-3_jpg.rf.d4d77069b0f99c01619476f1c0601a43.jpg: 640x384 1 face, 8.0ms\n","image 20/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-103_jpg.rf.c62318c668bafb8d4303e52c3a8dffbf.jpg: 640x384 1 face, 7.6ms\n","image 21/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-107_jpg.rf.4d47394dd4ff2fb1c0b9933c864961e6.jpg: 640x384 1 face, 8.2ms\n","image 22/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-113_jpg.rf.468f3e7209344dbe0bec99447ad59b78.jpg: 640x384 1 face, 7.9ms\n","image 23/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-127_jpg.rf.732c49818cd5891990133cf4143c9f6b.jpg: 640x384 1 face, 8.6ms\n","image 24/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-24_jpg.rf.84b385f329b2fcf27763dbcc44260e6c.jpg: 640x384 1 face, 9.4ms\n","image 25/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-31_jpg.rf.93052446268ce5956e2689ce8b429d2a.jpg: 640x384 1 face, 7.1ms\n","image 26/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-41_jpg.rf.daef2cbc4c15ee4e2703168878094703.jpg: 640x384 1 face, 11.9ms\n","image 27/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-57_jpg.rf.64b41b4767ad9ef545d8859669f9459f.jpg: 640x384 1 face, 8.2ms\n","image 28/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-5_jpg.rf.b1ae3a447870467d8964a3a327f7a5de.jpg: 640x384 1 face, 7.8ms\n","image 29/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-88_jpg.rf.3e83178a34e56e1ca21ff1a20a042c6d.jpg: 640x384 1 face, 7.9ms\n","image 30/164 /content/Face-Detection-1/test/images/IMG_4921-2_mp4-97_jpg.rf.bc92249757de5c051de084cf1dfe5e20.jpg: 640x384 1 face, 7.8ms\n","image 31/164 /content/Face-Detection-1/test/images/IMG_5490_mp4-11_jpg.rf.bb22e90930c899f2444749afaf3430f4.jpg: 640x384 (no detections), 7.8ms\n","image 32/164 /content/Face-Detection-1/test/images/IMG_5490_mp4-19_jpg.rf.0f989ce314c84b6d70a249235ecb98c0.jpg: 640x384 1 face, 7.6ms\n","image 33/164 /content/Face-Detection-1/test/images/IMG_5490_mp4-1_jpg.rf.5d42efe93938522d08c55702cf8be79a.jpg: 640x384 (no detections), 7.9ms\n","image 34/164 /content/Face-Detection-1/test/images/IMG_5491_mp4-3_jpg.rf.3c59d5356bbb116d1e10485d189f2779.jpg: 640x384 (no detections), 7.8ms\n","image 35/164 /content/Face-Detection-1/test/images/IMG_5491_mp4-8_jpg.rf.b7cf3e4e386fb0a9ef13543d91a8a09e.jpg: 640x384 1 face, 7.8ms\n","image 36/164 /content/Face-Detection-1/test/images/Image-from-iOS_MOV-31_jpg.rf.15f9099529e8d84647a3a9cb8e9c19ae.jpg: 640x384 1 face, 8.1ms\n","image 37/164 /content/Face-Detection-1/test/images/Inside-merge_mov-12_jpg.rf.d5d39d444e234d4ef8fdd62c6f3b1473.jpg: 640x384 (no detections), 8.8ms\n","image 38/164 /content/Face-Detection-1/test/images/Inside-merge_mov-21_jpg.rf.dfe802dc589be12c931e823220e32af3.jpg: 640x384 (no detections), 7.7ms\n","image 39/164 /content/Face-Detection-1/test/images/Inside-merge_mov-27_jpg.rf.b65ff2a0555ded4403c7db8a098370b8.jpg: 640x384 (no detections), 7.2ms\n","image 40/164 /content/Face-Detection-1/test/images/Inside-merge_mov-30_jpg.rf.6f6a53245a7ab6965920ccabe39287ff.jpg: 640x384 (no detections), 7.4ms\n","image 41/164 /content/Face-Detection-1/test/images/Inside-merge_mov-58_jpg.rf.7c85daab896f1d920dda6014bd89cbbc.jpg: 640x384 (no detections), 8.9ms\n","image 42/164 /content/Face-Detection-1/test/images/MTraore-photo_JPG.rf.25ec136a8dc9d6703a6933cbe7c439e8.jpg: 640x512 1 face, 60.2ms\n","image 43/164 /content/Face-Detection-1/test/images/Mask-detector1_mov-14_jpg.rf.e9168f9df0000efaa4956e9cb58f824f.jpg: 640x384 1 face, 8.5ms\n","image 44/164 /content/Face-Detection-1/test/images/Mask-detector1_mov-23_jpg.rf.27bd9413134d7ad59234fefa10000558.jpg: 640x384 1 face, 7.0ms\n","image 45/164 /content/Face-Detection-1/test/images/Mask-detector1_mov-27_jpg.rf.e38e527e25107792795c9a37befc7258.jpg: 640x384 1 face, 6.9ms\n","image 46/164 /content/Face-Detection-1/test/images/Mask-detector1_mov-53_jpg.rf.6237dc29e98a2915bf6ab133970573e0.jpg: 640x384 (no detections), 6.8ms\n","image 47/164 /content/Face-Detection-1/test/images/Mask2_mov-19_jpg.rf.8c08eda0c13030bfce5d67a810125ad7.jpg: 640x384 1 face, 6.8ms\n","image 48/164 /content/Face-Detection-1/test/images/Mask2_mov-38_jpg.rf.1fee4bc1dd73670dac1114c3d16976a3.jpg: 640x384 1 face, 6.8ms\n","image 49/164 /content/Face-Detection-1/test/images/Mask2_mov-52_jpg.rf.3886f56c3472b4a6829832519184184b.jpg: 640x384 1 face, 7.0ms\n","image 50/164 /content/Face-Detection-1/test/images/Mask2_mov-53_jpg.rf.3caf0d43dd5bbe6eed1e976289cbd6ec.jpg: 640x384 1 face, 7.1ms\n","image 51/164 /content/Face-Detection-1/test/images/Mask2_mov-56_jpg.rf.e8110a583c0eee1336773238cd4fdde1.jpg: 640x384 1 face, 6.9ms\n","image 52/164 /content/Face-Detection-1/test/images/Mask2_mov-62_jpg.rf.a5b93fd4610d0305b13ffc588d44b501.jpg: 640x384 1 face, 6.7ms\n","image 53/164 /content/Face-Detection-1/test/images/Mat-Hand-Signs_mov-128_jpg.rf.001c65bce55940a5cdc7e7a105a43e60.jpg: 384x640 2 faces, 7.7ms\n","image 54/164 /content/Face-Detection-1/test/images/Mat-Hand-Signs_mov-39_jpg.rf.6774aab97bd5675e7bebde5e58c843fa.jpg: 384x640 1 face, 7.0ms\n","image 55/164 /content/Face-Detection-1/test/images/Mohamed-Image-from-iOS_MOV-113_jpg.rf.1d40eab53fa011a6fa61dd88c50d308e.jpg: 640x384 1 face, 7.9ms\n","image 56/164 /content/Face-Detection-1/test/images/Mohamed-Image-from-iOS_MOV-26_jpg.rf.76efce95cf1435ad5f3553c091a4e15d.jpg: 640x384 1 face, 7.0ms\n","image 57/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0000_jpg.rf.cf8b361d62eef5487b1a8f1103c2dfeb.jpg: 448x640 1 face, 7.8ms\n","image 58/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0002_jpg.rf.9924e7b889c2020e6bfa7badadeb7d35.jpg: 448x640 1 face, 8.6ms\n","image 59/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0014_jpg.rf.b06d74af3d6ad9b39cf7a239865a47f7.jpg: 448x640 1 face, 10.2ms\n","image 60/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0053_jpg.rf.1d4c9937e2115300c54eda87404e1352.jpg: 448x640 2 faces, 12.3ms\n","image 61/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0055_jpg.rf.fc05fa08b4f6ff7d08dd8bd5c2efdca8.jpg: 448x640 2 faces, 8.1ms\n","image 62/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0056_jpg.rf.4ec5ee96a892b2d83801177b443d7e6f.jpg: 448x640 2 faces, 8.5ms\n","image 63/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0059_jpg.rf.49e3cad44df232e96623706454177f21.jpg: 448x640 3 faces, 10.7ms\n","image 64/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0092_jpg.rf.190fd2cc972a92898e9a58e0a87bc193.jpg: 448x640 2 faces, 8.4ms\n","image 65/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0100_jpg.rf.05112ae48637e3dc058cba986e78ac06.jpg: 448x640 1 face, 10.4ms\n","image 66/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0103_jpg.rf.8123cc1e37781ff2a3af0d257a437baa.jpg: 448x640 1 face, 7.7ms\n","image 67/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0128_jpg.rf.8365037c60305dc88a976e85b27b9b2e.jpg: 448x640 1 face, 12.0ms\n","image 68/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0143_jpg.rf.80d86b0791dc5f1469f63c591c55de43.jpg: 448x640 3 faces, 7.9ms\n","image 69/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0148_jpg.rf.ea8b0739c29274a62f3b8b8dafd86636.jpg: 448x640 2 faces, 7.3ms\n","image 70/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0152_jpg.rf.cae3697cf5c0b980f7e38681c7b0f9d3.jpg: 448x640 (no detections), 9.6ms\n","image 71/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0153_jpg.rf.5d4ed6b84c8dd10bb737949ba00c2745.jpg: 448x640 1 face, 9.5ms\n","image 72/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0155_jpg.rf.ab3f8473c324d9586a69fb657ffb3d29.jpg: 448x640 2 faces, 10.7ms\n","image 73/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0160_jpg.rf.8cbd73f39c253b5da958beeadcc4d504.jpg: 448x640 1 face, 8.6ms\n","image 74/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0180_jpg.rf.01757cd2017caabae401aec28e8f4c2d.jpg: 448x640 1 face, 7.6ms\n","image 75/164 /content/Face-Detection-1/test/images/Movie-on-2-18-25-at-8_25-PM_mov-0184_jpg.rf.da5982033f2374f83995cf1e7da7b2cb.jpg: 448x640 1 face, 14.8ms\n","image 76/164 /content/Face-Detection-1/test/images/PXL_20210930_151849553_jpg.rf.58e5a562f9b1c49af8688c2d3afa7960.jpg: 480x640 1 face, 12.0ms\n","image 77/164 /content/Face-Detection-1/test/images/PXL_20210930_151852090_jpg.rf.58dee3471e7906914d3a181fd07b8a2e.jpg: 480x640 1 face, 6.3ms\n","image 78/164 /content/Face-Detection-1/test/images/PXL_20210930_151854592_jpg.rf.02ee32d7c4b97f4cf9539a039b6066a6.jpg: 480x640 1 face, 6.3ms\n","image 79/164 /content/Face-Detection-1/test/images/PXL_20210930_152055464_jpg.rf.b452f1864081147d8742ef8bd4c344c4.jpg: 480x640 1 face, 6.1ms\n","image 80/164 /content/Face-Detection-1/test/images/Photo-on-10-26-21-at-2-33-PM-4-2_jpg.rf.ae6dc35a62a56b1ea6b892f4197ccc80.jpg: 448x640 (no detections), 6.7ms\n","image 81/164 /content/Face-Detection-1/test/images/Photo-on-10-26-21-at-2-33-PM_jpg.rf.7108105093cf6d84db0473f6302cb86f.jpg: 448x640 1 face, 5.9ms\n","image 82/164 /content/Face-Detection-1/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-36_jpg.rf.db9024c46a5f71f2494753428aa1fd42.jpg: 416x640 1 face, 6.6ms\n","image 83/164 /content/Face-Detection-1/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-3_jpg.rf.c142382344c4d4a2c9d64b135760e00d.jpg: 416x640 1 face, 8.7ms\n","image 84/164 /content/Face-Detection-1/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-96_jpg.rf.49b24f593a3ab9dae9d421972995bf6c.jpg: 416x640 1 face, 6.5ms\n","image 85/164 /content/Face-Detection-1/test/images/Stokoe20-58-scaled-1_jpg.rf.6c14274343c71246abe6488c8eaa066a.jpg: 640x640 7 faces, 7.9ms\n","image 86/164 /content/Face-Detection-1/test/images/ashton-video_mov-55_jpg.rf.7557129e3d98d8948bd91874165e88b8.jpg: 384x640 1 face, 6.3ms\n","image 87/164 /content/Face-Detection-1/test/images/download_jpeg_jpg.rf.d3e1089324696c15d4e2afa296395c20.jpg: 608x640 1 face, 37.0ms\n","image 88/164 /content/Face-Detection-1/test/images/face-detection_mp4-25_jpg.rf.3f2556cb76ed6b0f98395f34b6353a6f.jpg: 384x640 1 face, 6.4ms\n","image 89/164 /content/Face-Detection-1/test/images/face-detection_mp4-2_jpg.rf.a0eb35401ccf80d49cd55efbcf15bb28.jpg: 384x640 1 face, 5.6ms\n","image 90/164 /content/Face-Detection-1/test/images/face-detection_mp4-32_jpg.rf.7e992287643e882473a0a10195a36e2b.jpg: 384x640 1 face, 5.7ms\n","image 91/164 /content/Face-Detection-1/test/images/face-detection_mp4-4_jpg.rf.6c22549dbb12a0edc0eb92d8dfc2a203.jpg: 384x640 1 face, 5.7ms\n","image 92/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-102_jpg.rf.57bf82dea3391b51154e7974a9bc6a11.jpg: 480x640 1 face, 6.7ms\n","image 93/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-118_jpg.rf.926ebcc64cd361b6c8396586a31716fc.jpg: 480x640 1 face, 10.0ms\n","image 94/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-122_jpg.rf.91fba912b4ddb7ccb80dd289ce8ad4f7.jpg: 480x640 1 face, 11.7ms\n","image 95/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-126_jpg.rf.b438c824133987f0707772344e25dc8f.jpg: 480x640 (no detections), 6.4ms\n","image 96/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-135_jpg.rf.e1fc7af85a1c9732b829abf9e73594fd.jpg: 480x640 1 face, 5.8ms\n","image 97/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-13_jpg.rf.bfe1de0a859678a6065588e87930791e.jpg: 480x640 2 faces, 6.1ms\n","image 98/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-143_jpg.rf.5eedd19cc0afadd932a1ff39fd8b3d9b.jpg: 480x640 1 face, 7.1ms\n","image 99/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-148_jpg.rf.5be9104cee9091bdfc37fc7f0dbc7033.jpg: 480x640 1 face, 6.1ms\n","image 100/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-149_jpg.rf.e9c3465b3968e56c5b279290eb34f8e4.jpg: 480x640 2 faces, 6.9ms\n","image 101/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-20_jpg.rf.8930427acab5945eaacaa3f099c205d7.jpg: 480x640 2 faces, 5.9ms\n","image 102/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-30_jpg.rf.7453210e9c9dc2b1d42ad650b1dbd5ee.jpg: 480x640 2 faces, 5.8ms\n","image 103/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-33_jpg.rf.3a3d2b17a67e356b91c537901b4534bc.jpg: 480x640 2 faces, 5.9ms\n","image 104/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-39_jpg.rf.d9778d89990d95f36ad06f1af905e6f1.jpg: 480x640 2 faces, 7.9ms\n","image 105/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-3_jpg.rf.fed71eb2b1328509fbcaca370584efee.jpg: 480x640 1 face, 5.9ms\n","image 106/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-49_jpg.rf.1e25f1a582ca383f5dd49d0fa3fb7d0f.jpg: 480x640 2 faces, 5.9ms\n","image 107/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-92_jpg.rf.e508089b8076c568236366fcb0966420.jpg: 480x640 1 face, 6.5ms\n","image 108/164 /content/Face-Detection-1/test/images/gravitate-faces_mp4-9_jpg.rf.1c52fce256e0d25d7fb858ff2ff2d3a4.jpg: 480x640 1 face, 5.9ms\n","image 109/164 /content/Face-Detection-1/test/images/hand-signs-for-alex_mp4-57_jpg.rf.6257f85bb8f35d1b81e059dc0eea47e7.jpg: 384x640 2 faces, 8.0ms\n","image 110/164 /content/Face-Detection-1/test/images/istockphoto-1270066655-612x612_jpg.rf.449c027d20ca02a333c0de4f9d84e753.jpg: 448x640 4 faces, 6.7ms\n","image 111/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-10_jpg.rf.2e81a6490b6e460019c01b1b1d7b0086.jpg: 448x640 1 face, 6.1ms\n","image 112/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-21_jpg.rf.0349ce90c58e0964c5878856baa2a1cc.jpg: 448x640 1 face, 6.3ms\n","image 113/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-36_jpg.rf.0f91560572246e80da97950c8fe01ffd.jpg: 448x640 1 face, 5.9ms\n","image 114/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-38_jpg.rf.7f11add25e1feb84cc00585c486e98d3.jpg: 448x640 1 face, 5.8ms\n","image 115/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-50_jpg.rf.5d6fb3de5437ae7aabcb0c163d86c200.jpg: 448x640 1 face, 6.7ms\n","image 116/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-56_jpg.rf.cb4e611d5825c958819a33b7a6c8c2db.jpg: 448x640 1 face, 6.2ms\n","image 117/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-6_jpg.rf.1a8a037f565fca22e379bc8abefa2d03.jpg: 448x640 1 face, 5.9ms\n","image 118/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-71_jpg.rf.64a946852083b140c09bcfa5a9660435.jpg: 448x640 1 face, 5.7ms\n","image 119/164 /content/Face-Detection-1/test/images/mask-no-mask_mov-80_jpg.rf.c5f397568bf30f05097e9e85e698ce73.jpg: 448x640 1 face, 7.2ms\n","image 120/164 /content/Face-Detection-1/test/images/mask-wearing-1632931966866_png_jpg.rf.942e72a2e3e281f9fa3ca63ad2df4f32.jpg: 480x640 3 faces, 6.6ms\n","image 121/164 /content/Face-Detection-1/test/images/mask-wearing-1632932097427_png_jpg.rf.97dc6ce137a4bfdfa5cbf651217c5ce6.jpg: 480x640 3 faces, 5.8ms\n","image 122/164 /content/Face-Detection-1/test/images/mask-wearing-1632932267592_png_jpg.rf.30ca602f67e666f2888d29d2dcd06459.jpg: 480x640 1 face, 5.9ms\n","image 123/164 /content/Face-Detection-1/test/images/mask-wearing-1632933579037_png_jpg.rf.5890a0bf75dd9d6868200d6d17e7606f.jpg: 480x640 2 faces, 5.7ms\n","image 124/164 /content/Face-Detection-1/test/images/mask-wearing-1632937520400_png_jpg.rf.7cc3925b3318766bdc61b5eeed14d659.jpg: 480x640 1 face, 5.8ms\n","image 125/164 /content/Face-Detection-1/test/images/mask-wearing-1632937587369_png_jpg.rf.16a4acf92f8806125e402f568b5a376b.jpg: 480x640 1 face, 6.5ms\n","image 126/164 /content/Face-Detection-1/test/images/mask-wearing-1632937711552_png_jpg.rf.89e32146694582771a9274eae7e7cf05.jpg: 480x640 1 face, 5.8ms\n","image 127/164 /content/Face-Detection-1/test/images/mask-wearing-1632937724099_png_jpg.rf.f932745f96d5369bba410822b25a5a07.jpg: 480x640 1 face, 7.5ms\n","image 128/164 /content/Face-Detection-1/test/images/mask-wearing-1632937831241_png_jpg.rf.018047e7afb43de59f6bb2768a9c3fee.jpg: 480x640 1 face, 6.8ms\n","image 129/164 /content/Face-Detection-1/test/images/mask-wearing-1632937938545_png_jpg.rf.d1df3bcf4edf8958fe036a18bf33f376.jpg: 480x640 1 face, 5.7ms\n","image 130/164 /content/Face-Detection-1/test/images/mask-wearing-1632938044674_png_jpg.rf.536b0ae3332216357d502f71757d9ff3.jpg: 480x640 1 face, 5.7ms\n","image 131/164 /content/Face-Detection-1/test/images/mask-wearing-1632938112706_png_jpg.rf.c181abdd1af064b535c2bddbe7e39a80.jpg: 480x640 1 face, 5.7ms\n","image 132/164 /content/Face-Detection-1/test/images/mask-wearing-1632938125416_png_jpg.rf.8aad688a206a7923ff2d5e5eeb6a80d8.jpg: 480x640 1 face, 5.7ms\n","image 133/164 /content/Face-Detection-1/test/images/mask-wearing-1632939814594_png_jpg.rf.277bfe52862020b71642d8aea8fa05d1.jpg: 480x640 1 face, 7.5ms\n","image 134/164 /content/Face-Detection-1/test/images/mask-wearing-1632939984654_png_jpg.rf.ca1e4bc76b4179d9c241ee444a20e355.jpg: 480x640 1 face, 5.9ms\n","image 135/164 /content/Face-Detection-1/test/images/mask_mov-10_jpg.rf.6ae8fbca83625bb281f77054f1fc39cb.jpg: 448x640 1 face, 6.5ms\n","image 136/164 /content/Face-Detection-1/test/images/mask_mov-14_jpg.rf.e6553758341697c01bd6259440b3279b.jpg: 448x640 1 face, 5.7ms\n","image 137/164 /content/Face-Detection-1/test/images/mo-justin-mask-NoMask_mov-1_jpg.rf.aa67a0a37e3b46e66090ab45433c4f4a.jpg: 640x576 2 faces, 39.1ms\n","image 138/164 /content/Face-Detection-1/test/images/no-mask_mov-17_jpg.rf.1d635b3ec83ac9c7c86c449014b5f132.jpg: 448x640 1 face, 6.5ms\n","image 139/164 /content/Face-Detection-1/test/images/no-mask_mov-19_jpg.rf.dda6f8f133c6526e0e4e297700d1b7c2.jpg: 448x640 1 face, 5.8ms\n","image 140/164 /content/Face-Detection-1/test/images/no-mask_mov-21_jpg.rf.69d707be16367e8645bbc6c696d14ad6.jpg: 448x640 1 face, 5.7ms\n","image 141/164 /content/Face-Detection-1/test/images/no-mask_mov-25_jpg.rf.b395216ee00faa36e36e2c296d41a67a.jpg: 448x640 1 face, 5.8ms\n","image 142/164 /content/Face-Detection-1/test/images/no-mask_mov-2_jpg.rf.ac4dc7957d39b45edf0e65d97b08bba8.jpg: 448x640 1 face, 6.9ms\n","image 143/164 /content/Face-Detection-1/test/images/no-mask_mov-3_jpg.rf.9ab43f57460962d6c82edc47cc0e4af9.jpg: 448x640 1 face, 10.1ms\n","image 144/164 /content/Face-Detection-1/test/images/no-mask_mov-5_jpg.rf.8ccd66111d66e10cc25f485f10d6c0ce.jpg: 448x640 1 face, 5.6ms\n","image 145/164 /content/Face-Detection-1/test/images/phplpE73q_jpg.rf.c779d462a668ca9a8471b0d21df98c1b.jpg: 448x640 47 faces, 5.6ms\n","image 146/164 /content/Face-Detection-1/test/images/r1000019q679o5611r7_jpg.rf.be78033cd946d3874b9b9e1e05b1cb3e.jpg: 640x448 2 faces, 39.0ms\n","image 147/164 /content/Face-Detection-1/test/images/r1p00017o8357s6sno6_jpg.rf.3ccf308b12cd60a1f9c544f37ad0c87e.jpg: 640x448 1 face, 5.6ms\n","image 148/164 /content/Face-Detection-1/test/images/rally-against-an-anti-mask-law-meant-to-deter-anti-government-protesters-in-hong-kong-china-shutterstock-editorial-10435716z_jpg.rf.3258c42c4694a905cd328333c97029af.jpg: 448x640 18 faces, 6.5ms\n","image 149/164 /content/Face-Detection-1/test/images/roboflow-44_jpg.rf.5aaf51d3200054d1a85f9da79864def7.jpg: 640x448 1 face, 8.1ms\n","image 150/164 /content/Face-Detection-1/test/images/sdfsdfsfff_jpg.rf.7e68ec7cdbb19279eb8ec8d5f7634429.jpg: 384x640 1 face, 9.1ms\n","image 151/164 /content/Face-Detection-1/test/images/stsciRq_png_jpg.rf.93059501486d214cf290c5fa9883da8b.jpg: 640x320 2 faces, 37.3ms\n","image 152/164 /content/Face-Detection-1/test/images/unnamed_jpg.rf.3208d68ff075b17c75541b5e4a14e209.jpg: 448x640 16 faces, 6.4ms\n","image 153/164 /content/Face-Detection-1/test/images/w1240-p16x9-0e48e0098f6e832f27d8b581b33bbc72b9967a63_jpg.rf.21558ffb16976af00bfb28edad086005.jpg: 384x640 3 faces, 6.2ms\n","image 154/164 /content/Face-Detection-1/test/images/youtube-12_jpg.rf.49b81177513a4646cccaba43b3212e06.jpg: 384x640 (no detections), 6.2ms\n","image 155/164 /content/Face-Detection-1/test/images/youtube-16_jpg.rf.a00da1efb71e3413b3b0cea09df128c1.jpg: 384x640 (no detections), 6.0ms\n","image 156/164 /content/Face-Detection-1/test/images/youtube-43_jpg.rf.d9c33e1a36434c2c31d23b0c2c6bc767.jpg: 384x640 (no detections), 5.7ms\n","image 157/164 /content/Face-Detection-1/test/images/youtube-46_jpg.rf.a5e878bd7b116b5e3db1c00c4de52a03.jpg: 384x640 (no detections), 5.7ms\n","image 158/164 /content/Face-Detection-1/test/images/youtube-72_jpg.rf.79254b707d7553b71c39cd4a069cd3d5.jpg: 384x640 (no detections), 5.6ms\n","image 159/164 /content/Face-Detection-1/test/images/youtube-74_jpg.rf.022ab201c8ac84bbe8774d2caaa83e72.jpg: 384x640 1 face, 7.7ms\n","image 160/164 /content/Face-Detection-1/test/images/youtube-75_jpg.rf.f5a705bed1d7fbc8cf196ab80c2a178b.jpg: 384x640 (no detections), 5.8ms\n","image 161/164 /content/Face-Detection-1/test/images/youtube-76_jpg.rf.d3dc5d36c73c3a37d27f9b4641621f51.jpg: 384x640 (no detections), 5.8ms\n","image 162/164 /content/Face-Detection-1/test/images/youtube-80_jpg.rf.997d7f463d06361dc1d80bf8cc4db5d3.jpg: 384x640 (no detections), 6.9ms\n","image 163/164 /content/Face-Detection-1/test/images/youtube-83_jpg.rf.d3fd42051b386ee0c7c57d9b64b3b2b1.jpg: 384x640 (no detections), 5.8ms\n","image 164/164 /content/Face-Detection-1/test/images/youtube-86_jpg.rf.07a41a5a8a3ac6ebb344d7730391212f.jpg: 384x640 (no detections), 6.3ms\n","Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jOFVQGBja1","executionInfo":{"status":"ok","timestamp":1756910852766,"user_tz":-345,"elapsed":22849,"user":{"displayName":"Bishes Maharjan","userId":"07718392229561475250"}},"outputId":"cd75965f-9026-41ab-9b05-631d8c48f335"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-kCazknEC7_S"},"execution_count":null,"outputs":[]}]}